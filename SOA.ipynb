{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9285a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/DL_Term_Paper/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get Model\n",
    "\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"farleyknight-org-username/vit-base-mnist\")\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"farleyknight-org-username/vit-base-mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4311e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d4c4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "\n",
    "from Data.Data import DataLoader\n",
    "\n",
    "distortions = ['shot_noise', 'motion_blur']  # desired distortions\n",
    "\n",
    "test_data_obj = DataLoader('test', distortions)\n",
    "\n",
    "# duplicates encoutered in create_dataset function so just using full testing dataset for now\n",
    "\n",
    "'''test_size = 1000\n",
    "\n",
    "#test_ratios = {'clean': 0.2, 'shot_noise': 0.4, 'motion_blur': 0.6}\n",
    "\n",
    "#test_dataset = test_data_obj.create_dataset(test_size, test_ratios)\n",
    "'''\n",
    "\n",
    "test_data_dict = test_data_obj.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd4a0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = test_data_dict['clean'][:100]\n",
    "#images.extend(test_data_dict['shot_noise'][:100])\n",
    "#images.extend(test_data_dict['motion_blur'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95089ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([28, 28, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# orginal dimensions\n",
    "og_images = images.copy()\n",
    "og_images[0]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47d382af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Pytorch Tensors and appropriate dimensions based on https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "for image in images:\n",
    "    numpy_array = image['image'].numpy() / 255 # turn tensorflow tensor to numpy array\n",
    "    \n",
    "    # move channel dimension to the front and convert into Pytorch tensor\n",
    "    tensor = torch.tensor(numpy_array.reshape((1, 28, 28)))\n",
    "    \n",
    "    tensor = tensor.unsqueeze(0) # add batch size dimesion at index 0\n",
    "    \n",
    "    # expand image from 28x28 to 224x244\n",
    "    tensor = F.interpolate(tensor, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    # make the tensor have three channels instead of 1\n",
    "    final_tensor = torch.cat((tensor, tensor, tensor), dim=1)\n",
    "    \n",
    "    image['image'] = final_tensor\n",
    "    \n",
    "    # convert label tensorflow tensor to Pytorch tensor\n",
    "    image['label'] = torch.tensor(image['label'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e555903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new dimensions [batch_size, channels, height, width]\n",
    "images[0]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "939a5a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb9ecc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss, correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    for image in range(len(images)):\n",
    "        \n",
    "        pred = model(images[image]['image'])\n",
    "        \n",
    "        correct += (pred.logits.argmax(1) == images[image]['label']).type(torch.float).sum().item()\n",
    "        \n",
    "    print(correct/len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cdcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
